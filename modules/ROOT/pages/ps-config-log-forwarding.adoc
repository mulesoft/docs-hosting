= Configuring Logging for a Private Space 

include::partial$feature-availability.adoc[tag=featureUnavailableGA]



*This topic is updated to the Design Spec dated 24-Nov-21*

*Last update* 10-Feb-22

*This will change based on the updates to the log forwarding capabilities*

// How can we share this content with RTF?

Use options on the *Logging* tab to:

* Configure log forwarding for the private space.
+
CloudHub 2.0 supports forwarding logs to Anypoint Monitoring and several external services.
+
The log forwarding that you configure for the private space define the options that are available 
when deploying apps to the private spaces.
See xref:ch2-config-endpoints-paths.adoc[].
+
You can also select to forward ingress load balancer logs to the configured external service.
* Specify the default log level for ingress load balancer logs and configure custom log levels.
+
You can configure a custom log level for a specific IP address to help troubleshoot issues.



[[configure-log-forwarding]]
== Configure Log Forwarding

CloudHub 2.0 supports log forwarding to:

* Anypoint Monitoring
+
Forwards only application logs to Anypoint Monitoring and requires a Titanium subscription
* External services 
+
Forwards application and load-balancer logs to the specified service for external analysis:
+
** Azure
** Elasticsearch
** Graylog (GELF)
** Splunk
** CloudWatch


== Before You Begin 

To forward logs to:

* Anypoint Monitoring, ensure you have a Titanium subscription.
* External services:
+
.. Configure the external service.
.. Configure firewall rules to allow outbound traffic for TCP.
+
See xref:ps-config-fw-rules.adoc[]

=== Splunk Prerequisites 

To send data to Splunk using the HTTP Event Collector, you must first set up the data input and obtain a token from Splunk:

. Sign in to your Splunk account.
. Navigate to *Settings* > *Data Inputs*.
. For the *HTTP Event Collector* type, click *Add new*.
+
.The arrow shows the *Add new* option for the HTTP Event Collector on the Splunk Data Inputs page.
image::splunk-config-http-collector.png[Add new option for the HTTP Event Collector on the Splunk Data Inputs page]

. Follow the steps to set up the data input and obtain the token.

////
To send data to Splunk over TCP, you must first enable the input source in Splunk:

. Sign in to your Splunk account.
. Navigate to *Settings* > *Data Inputs*.
. For the *TCP* type, click *Add new*.
+
.The arrow shows the *Add new* option for TCP on the Splunk Data Inputs page.
image::splunk-config-tcp.png[Add new option for TCP on the Splunk Data Inputs page]
. Follow the steps to set up the data input.

////

For more information about setting up data inputs, see the https://docs.splunk.com/Documentation/Splunk/8.0.2/Data/WhatSplunkcanmonitor[Splunk documentation^]. (Link out from Beta docs) 

//	Specify Load Balancer Log Level		
//	Specify RTFC Log Level		
//  Configure Internal DNS to Resolve Private Domains			
// 	Update DNS Server with Private Domains		

== Configure Log Forwarding 


//SELECT PRIVATE SPACE SHARED
include::partial$select-private-space.adoc[tag=selectPrivateSpace]
include::partial$select-private-space.adoc[tag=clickPrivateSpaceName]
. Click the *Logging* tab.
. Select logging options:
+
--
** *Show logs in Anypoint Monitoring*
+
This option is available only if you have a Titanium subscription.
** *Forward logs to an external service*
+
.. Select the service and enter the configuration information for the service:
+
*** <<azure-parameters>> 
*** <<elasticsearch-parameters>>
*** <<gelf-parameters>> 
*** <<splunk-parameters>>
*** <<cloudwatch-parameters>>

.. Select to forward logs from:

*** *Mule Applications*
+
You can disable log forwarding for an app during deployment.
*** *CloudHub 2.0 components and Ingress load balancer*
*** *Monitoring and appliance services*
--
. Click *Save logging* or *Discard changes*.
+
The *Logging* page displays a message indicating that the 
logging configuration is applied.
. Click *Send a test message* to test the connection
between your private space and the external service.


[[azure-parameters]]
=== Azure Log Analytics Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Azure Log Analytics Configuration Parameters
|===
| Parameter | Description | Required | Default Value
| Customer ID | Specifies the customer ID or workspace ID string. | Yes |
| Shared Key | Specifies the primary or secondary connected sources client authentication key.

This value is hidden and stored securely.
 | Yes |
| Log Type | Specifies the event type name. | No | runtime_fabric
|===


[[elasticsearch-parameters]]
=== Elasticsearch Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Elasticsearch Configuration Parameters
|===
| Parameter | Description | Required | Default Value
| Host | Specifies the IP address or hostname of the target Elasticsearch instance. | Yes | 127.0.0.1
| Port | Specifies the TCP port of the target Elasticsearch instance. | Yes | 9200
| Index | Specifies the index name. | Yes | runtime_fabric
| Path | Elasticsearch accepts new data on HTTP query path `/_bulk`. You can also serve Elasticsearch behind a reverse proxy on a subpath. This option defines such a path on the output plugin side. It adds a path prefix in the indexing HTTP POST URI. | No | Empty string

| Buffer Size | Specifies the buffer size used to read the response from the Elasticsearch HTTP service.

This option is useful for debugging purposes where reading the full response is needed.
The size of the response grows depending on the number of records inserted. To specify an unlimited amount of memory, set this value to `False`. Otherwise, set the value according to the Unit Size specification. | No | 4KB
| HTTP User | Specifies an optional username credential for Elastic X-Pack access. | No | 
| HTTP Password | Specifies a password for the user defined in `HTTP User`. | No | 
| Type | Specifies the type name. | No | runtime_fabric_cloud
| Logstash Format | Enables Logstash format compatibility.

This option takes a Boolean value: `True`, `False`, `On`, or `Off`. | No |
| Logstash Prefix | When `Logstash Format` is enabled, the `Index` name is composed of a prefix and the date. For example, if `Logstash Prefix` is set to `mydata`, your index becomes `mydata-YYYY.MM.DD`. The last string that is appended belongs to the date when the data is generated. | No | logstash
| Logstash Date Format | Specifies the time format (based on `strftime`) that is used to generate the second part of the `Index` name. | No | %Y.%m.%d
| Time Key | When `Logstash Format` is enabled, each record is assigned a new timestamp field. The `Time Key` property defines the name of that field. | No | @timestamp
| Time Key Format | When `Logstash Format` is enabled, this property defines the format of the timestamp. | No | %Y-%m-%dT%H:%M:%S
| Include Tag Key | When enabled, the tag name is appended to the record. | No | Off
| Tag Key | When `Include Tag Key` is enabled, this property defines the key name for the tag. | No |
| Generate ID | When enabled, generates the `_id` value for outgoing records. This prevents duplicate records when retrying ES. | No | Off
| Replace Dots | When enabled, replaces field name dots (`.`) with an underscore (`_`), which is required by Elasticsearch 2.0 through 2.3. | No | Off
| Trace Output | When enabled, prints the Elasticsearch API calls to stdout (for diagnosis). | No | Off
| Trace Error | When enabled, prints the Elasticsearch API calls to stdout when Elasticsearch returns an error. | No | Off
| Current Time Index | Specifies using the current time for index generation instead of message record information. | No | Off
| Logstash Prefix Key | Prefixes keys with this string. | No |
| TLS | Enables or disables TLS support. | No | Off
| CA Path Certificate File |  Specifies the CA certificate file to be uploaded. This option is available only when TLS is enabled. | No |
|===


[[gelf-parameters]]
=== GELF Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.GELF Configuration Parameters
|===
| Key | Description | Required | Default Value
| Match | Specifies the pattern to match for log tags to be output by this plugin. | Yes |
| Host | Specifies the IP address or hostname of the target Graylog server. | Yes | 127.0.0.1
| Port | Specifies the port on which your Graylog GELF input is listening. | Yes | 12201
| Mode | Specifies the protocol to use (`tls`, `tcp`, or `udp`). | Yes | udp
| Gelf Short Message Key | Specifies a short descriptive message.

This value must be set in GELF. | Yes | log
| Gelf Timestamp Key | Specifies your log timestamp.

This value should be set in GELF. | No | timestamp
| Gelf Host Key | Specifies the key for the value that is used as the name of the host, source, or application that sent the message.

This value must be set in GELF. | No | host
| Gelf Full Message Key | Specifies the key to use as the long message, which can contain a backtrace.

This value is optional in GELF. | No | full_message
| Gelf Level Key | Specifies the key to be used as the log level.

The key value must be a standard syslog level (between 0 and 7). | No | level
| Packet Size | Specifies the size of packets to be sent if the transport protocol is set to `udp`. | No | 1420
| Compress | Specifies compression of your UDP packets if the transport protocol is set to `udp`. | No | true
| TLS | Enables or disables TLS support. | No | Off
| CA Path Certificate File |  Specifies the CA certificate file to be uploaded.

This option is available only when TLS is enabled. | No |
|===

Example output plugin configuration:
```
[OUTPUT]
    Name                    gelf
    Match                   kube.*
    Host                    <your-graylog-server>
    Port                    12201
    Mode                    tcp
    Gelf_Short_Message_Key  log
```

[[splunk-parameters]]
=== Splunk Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Splunk Configuration Parameters
|===
| Parameter | Description | Required | Default Value
| Host | Specifies the IP address or hostname of the target Splunk service. | Yes | 127.0.0.1
| Port | Specifies the TCP port of the target Splunk service. | Yes | 8088
| Splunk Token | Specifies the Authentication Token for the HTTP Event Collector interface.

CloudHub 2.0 hides and stores this value securely. | Yes |
| HTTP User | Specifies a username for basic authentication on the HTTP Event Collector. | No |
| HTTP Password | Specifies a password for the user defined in HTTP user.

CloudHub 2.0 hides and stores this value securely. | No |
| TLS | Enables or disables TLS support. | No | Off
| CA Path Certificate File |  Specifies the CA certificate file to upload.

This option is available only when TLS is enabled. | No |
|===

[[cloudwatch-parameters]]
==== CloudWatch Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.CloudWatch Configuration Parameters
|===
| Key | Description | Required | Default Value
| Region | Specifies the AWS region. | Yes |
| AWS Access Key ID | Specifies the access key ID for the IAM user. | Yes |
| AWS Secret Access Key | Specifies the AWS secret access key for the IAM user. | Yes |
| Log Group Name | Specifies the name of the CloudWatch log group to which you want log records sent.| Yes | runtime_fabric_cloudwatch
| Log Stream Name | Specifies the name of the CloudWatch log stream to which you want log records sent. | Yes |
| Log Stream Prefix | Specifies the prefix for the log stream name.

The tag is appended to the prefix to construct the full log stream name. Not compatible with the `Log Stream Name` option. | No |
| Role ARN | Specifies the ARN of an IAM role to assume (for cross account access). | No |
| Auto Create Group | Automatically creates the log group.

Valid values are `On` or `Off` (case sensitive). | No | Off
|===

[NOTE]
You must specify either  `Log Stream Name` or `Log Stream Prefix`, but not both.

Example output plugin configuration:
```
[OUTPUT]
    Name                   cloudwatch_logs
    region                 us-east-1
    aws_access_key_id      <your_AWS_access_key_ID>
    aws_secret_access_key  <your_AWS_secret_access_key>
    log_group_name         runtime_fabric_cloudwatch
    log_stream_prefix      my_stream
    auto_create_group      On
```

////

[[syslog-parameters]]
==== Syslog Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Syslog Configuration Parameters
|===
| Key | Description | Required | Default Value
| Host | Host name or IP address of the remote Syslog server | Yes |
| Port | TCP or UDP port of the remote Syslog server | Yes | 514
| Mode | The mode of the transport type | Yes | udp
| Syslog format | Syslog protocol format | No | rfc5424
| Syslog Maxsize | Integer value that sets the maximum number of bytes per message | No | 1024 for rfc3164, 2048 for rfc5424
|===

Example output plugin configuration:
```
[OUTPUT]
    Name           syslog
    Host           127.0.0.1
    Port           514
    Mode           udp
    Syslog_Format  rfc5424
    Syslog_Maxsize 2048
```

[[datadog-parameters]]
==== Datadog Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Datadog Configuration Parameters
|===
| Key | Description | Required | Default Value
| Host | The host name of the remote Datadog server. | Yes |
| Api Key | The Datadog API key. | Yes |
| Compress | Compresses the payload in GZIP format. Datadog recommends setting this to `gzip`. | No | `gzip`
| Datadog Service | The name of the service generating the logs. For example, `runtime_fabric` | No |
| Datadog Source | The type of service. For example, `postgres` or `nginx` | No |
| Datadog Tags | The tags assigned to the logs in Datadog. | No |
| TLS | End-to-end secure communications protocol support. DataDog does not support CA certificate files.| No | Off
|===

Example output plugin configuration:
```
[OUTPUT]
    Name        datadog
    Host        http-intake.logs.datadoghq.com
    apikey      <your-datadog-api-key>
    TLS         on
    compress    gzip
    dd_service  <your-app-service>
    dd_source   <your-app-source>
    dd_tags     team:logs,foo:bar
```
////
[[delete-config]]
== Delete the External Logging Configuration

To delete the configuration, deselect *Forward logs to an external service*
and then click *Save logging*. 



== See Also 

* xref:ps-config-domains.adoc[]
* xref:ch2-deploy-private-space.adoc[]